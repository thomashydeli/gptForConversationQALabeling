{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries for data loading, tagging and presentation\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import clear_output\n",
    "# load open AI API keys\n",
    "with open('../../../secrets.json','r') as f:\n",
    "    secrets=json.load(f)\n",
    "openai.api_key=secrets['openai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data, in this personal practice project, we used data from:\n",
    "# Software-related-Slack-Chats-with-Disentangled-Conversations [https://github.com/preethac/Software-related-Slack-Chats-with-Disentangled-Conversations]\n",
    "# special thanks to the contributors: Kostadin Damevski and Preetha Chatterjee\n",
    "\n",
    "# loading data in subfolders and pack into one unfiromed dataframe for text concatenation\n",
    "encoded_names=[]\n",
    "directories=[]\n",
    "for folder in os.listdir('data/raw_data/data'):\n",
    "    subfolders=os.listdir(f'data/raw_data/data/{folder}')\n",
    "    for subfolder in subfolders:\n",
    "        files=os.listdir(f'data/raw_data/data/{folder}/{subfolder}')\n",
    "        for i,f in enumerate(files):\n",
    "            directories.append(f'data/raw_data/data/{folder}/{subfolder}/{f}')\n",
    "            encoded_names.append(f'{folder}-{subfolder}-{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe spaned by conversation_id, user, and text\n",
    "DataFrame=pd.DataFrame()\n",
    "for encoded_name, directory in zip(encoded_names, directories):\n",
    "    with open(directory,'r',encoding='utf8') as f:\n",
    "        data=f.read()\n",
    "        \n",
    "    bs_data=BeautifulSoup(data,'xml')\n",
    "    messages=bs_data.find_all('message')\n",
    "\n",
    "    dataframe={\n",
    "        'conversation_id':[],\n",
    "        'user':[],\n",
    "        'text':[],\n",
    "    }\n",
    "\n",
    "    # using BeautifulSoup to get message, user info, as well as creation of conversation ID\n",
    "    for message in messages:\n",
    "        dataframe['conversation_id'].append(encoded_name+'-'+message.get('conversation_id'))\n",
    "        text=message.find_all('text')\n",
    "        dataframe['user'].append(message.find('user').text)\n",
    "        if len(text) > 1: # if more than one text tag identified, append all content\n",
    "            text2append=''\n",
    "            for t in text:\n",
    "                text2append+=t.text\n",
    "            dataframe['text'].append(text2append)\n",
    "        elif len(text) < 1:\n",
    "            dataframe['text'].append('') # if nothing identified under text tag, then just append an empty string\n",
    "        else:\n",
    "            dataframe['text'].append(text[0].text)\n",
    "    dataframe=pd.DataFrame(dataframe)\n",
    "    DataFrame=pd.concat([dataframe, DataFrame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab text data by joining all converation text corresponding to the same converstaion ID\n",
    "text_data=[]\n",
    "for conversation in tqdm(DataFrame.conversation_id.unique()):\n",
    "    subset=DataFrame[DataFrame['conversation_id']==conversation]\n",
    "    text=[]\n",
    "    for _,row in subset.iterrows():\n",
    "        text.append(f\"{row['user']}: {row['text']}\")\n",
    "    text=\"\\n\\n\".join(text)\n",
    "    text_data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearraging and shuffling text_data, so that we can perform data tagging not in sequential fashion, i.e.\n",
    "# not just conversation regarding one technique will be parsed if something was broken in the loop\n",
    "text_data=list(np.array(text_data)[np.random.choice(len(text_data), len(text_data), replace=False)])\n",
    "with open('data/text_data/text_data.pickle','wb') as f:\n",
    "    pickle.dump(text_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can start from here to load raw text data from now on\n",
    "with open('data/text_data/text_data.pickle','rb') as f:\n",
    "    text_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt skelton\n",
    "base_prompt=\"\"\"The follow text snippet is a conversation between multiple individuals asking some technical questions:\\n\"\"\"\n",
    "# introduces this is a conversation regarding technical questions\n",
    "\n",
    "question_plugin=\"\"\"\n",
    "Based on this conversation, the question that was raised was: {question}\"\"\"\n",
    "# prompt to identify the raised question in the conversation\n",
    "\n",
    "question_summary_plugin=f\"\"\"\n",
    "Use three top keywords to summarize this question (just give me the words separated by comma): \"\"\"\n",
    "# prompt for summarizing what question was raised into 3 keywords\n",
    "\n",
    "answer_plugin=\"\"\"\n",
    "The answer to the question was: {answer}\"\"\"\n",
    "# prompt for getting the answers\n",
    "\n",
    "answer_summary_plugin=f\"\"\"\n",
    "Use three top keywords to summarize the answer (just give me the words separated by comma): \"\"\"\n",
    "# prompt for summarizing what answer was proposed into 3 keywords\n",
    "\n",
    "question_user_plugin=\"\"\"\n",
    "The person or people who asked the question was/were (just give me his/her/their name(s) separated by comma): {asker}\"\"\"\n",
    "# prompt asking who asked the question\n",
    "\n",
    "answer_user_plugin=\"\"\"\n",
    "The person or people who answered the question was/were (just give me his/her/their name(s) separated by comma): {answerer}\"\"\"\n",
    "# prompt asking who answer the question\n",
    "\n",
    "answer_rating_plugin=f\"\"\"\n",
    "How would you raise this answer on a scale from 0-10 (just give me the score): \"\"\"\n",
    "# prompt for rating the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price estimation\n",
    "# added a 1.2X discount to counter for potential price overflow due to underestimation\n",
    "# unit price default set to the price for 1k tokens using gpt-3.5-turbo\n",
    "def priceEstimator(prompt, response, unit_price=0.002, discount=1.2):\n",
    "    prompt_price=len(prompt.split())*discount*unit_price/1000\n",
    "    response_price=len(response.split())*discount*unit_price/1000\n",
    "    return prompt_price+response_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to GPT with prompt, token to complete, as well as a very low temperature to make sure deterministic of the answers\n",
    "def send2GPT(prompt, maxlen=1500):\n",
    "    estimated_token=min(4097-int(np.floor(len(prompt.split())*1.5)),maxlen)\n",
    "    # estimated token for completion, this was updated\n",
    "    # 1.5X applied to counter for potential gap between estimation and actual identified tokens by OpenAI\n",
    "    output=getResponse(\n",
    "        prompt, \n",
    "        estimated_token,\n",
    "        0.01\n",
    "    ).lstrip().rstrip()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting responses by calling OpenAI ChatCompletion API\n",
    "def getResponse(prompt, max_tokens, temperature=0.2):\n",
    "    completion=openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ], # chat completion needs to provide role and content in an array\n",
    "        max_tokens=max_tokens,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    response=completion.choices[0]['message']['content'].strip() # getting the output message\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling OpenAI for data distillation\n",
    "qa_data={\n",
    "    'question':[],\n",
    "    'asker':[],\n",
    "    'question_summary':[],\n",
    "    'answer':[],\n",
    "    'answerer':[],\n",
    "    'answer_summary':[],\n",
    "    'was_resolved':[],\n",
    "    'answer_rating':[],\n",
    "    'conversation':[],\n",
    "}\n",
    "total_pricing=0 # total price so far\n",
    "i=0\n",
    "current_unixtime=int(datetime.now().timestamp())\n",
    "while ((total_pricing < 50) & (i < 15000)): # default set as $50 and 15,000 conversations to begin with\n",
    "    print(f'[INFO] currently processing item: {i+1}')\n",
    "    conversation=\"\\n```\\n\"+text_data[i]+\"\\n```\\n\"\n",
    "    pricing=0\n",
    "\n",
    "    # what is the question?\n",
    "    prompt=base_prompt+conversation+question_plugin.format(\n",
    "        question=''\n",
    "    )\n",
    "    print(f'[DEBUG] question prompt: {prompt}')\n",
    "    question=send2GPT(prompt)\n",
    "    print(f'[DEBUG] question identified: {question}')\n",
    "    print()\n",
    "    pricing+=priceEstimator(prompt, question)\n",
    "    \n",
    "    # who asked the question?\n",
    "    prompt=base_prompt+conversation+question_plugin.format(\n",
    "        question=question\n",
    "    )+'\\n'+question_user_plugin.format(\n",
    "        asker=''\n",
    "    )\n",
    "    print(f'[DEBUG] asker prompt: {prompt}')\n",
    "    asker=send2GPT(prompt)\n",
    "    print(f'[DEBUG] asker identified: {prompt}')\n",
    "    print()\n",
    "    pricing+=priceEstimator(prompt, asker)\n",
    "    \n",
    "    # question summary\n",
    "    prompt=base_prompt+conversation+question_plugin.format(\n",
    "        question=question\n",
    "    )+'\\n'+question_summary_plugin\n",
    "    print(f'[DEBUG] question summary prompt: {prompt}')\n",
    "    question_summary=send2GPT(prompt)\n",
    "    print(f'[DEBUG] question summary identified: {question_summary}')\n",
    "    print()\n",
    "    pricing+=priceEstimator(prompt, question_summary)\n",
    "    \n",
    "    # what is the answer?\n",
    "    prompt=base_prompt+conversation+question_plugin.format(\n",
    "        question=question\n",
    "    )+'\\n'+answer_plugin.format(answer='')\n",
    "    print(f'[DEBUG] answer prompt: {prompt}')\n",
    "    answer=send2GPT(prompt)\n",
    "    print(f'[DEBUG] answer identified: {answer}')\n",
    "    print()\n",
    "    pricing+=priceEstimator(prompt, answer)\n",
    "    \n",
    "    # who answered the question?\n",
    "    prompt=base_prompt+conversation+question_plugin.format(\n",
    "        question=question\n",
    "    )+'\\n'+answer_user_plugin.format(\n",
    "        answerer=''\n",
    "    )\n",
    "    print(f'[DEBUG] answerer prompt: {prompt}')\n",
    "    answerer=send2GPT(prompt)\n",
    "    print(f'[DEBUG] answerer identified: {answerer}')\n",
    "    print()\n",
    "    pricing+=priceEstimator(prompt, answerer)\n",
    "    \n",
    "    # answer summary\n",
    "    prompt=base_prompt+conversation+question_plugin.format(\n",
    "        question=question\n",
    "    )+'\\n'+answer_plugin.format(\n",
    "        answer=answer\n",
    "    )+'\\n'+answer_summary_plugin\n",
    "    print(f'[DEBUG] answer summary prompt: {prompt}')\n",
    "    answer_summary=send2GPT(prompt)\n",
    "    print(f'[DEBUG] answer summary identified: {answer_summary}')\n",
    "    print()\n",
    "    pricing+=priceEstimator(prompt, answer_summary)\n",
    "    \n",
    "    # was resolved?\n",
    "    prompt=base_prompt+conversation+question_plugin.format(\n",
    "        question=question\n",
    "    )+'\\n'+answer_plugin.format(\n",
    "        answer=answer\n",
    "    )+'\\n'+\"Was the question/issue resolved (only answer Yes or No, and answer No if the answer is vague or unclear):\"\n",
    "    print(f'[DEBUG] was resolved prompt: {prompt}')\n",
    "    was_resolved=send2GPT(prompt)\n",
    "    print(f'[DEBUG] was resolved identifed: {was_resolved}')\n",
    "    print()\n",
    "    pricing+=priceEstimator(prompt, was_resolved)\n",
    "    \n",
    "    # answer rating\n",
    "    prompt=base_prompt+conversation+question_plugin.format(\n",
    "        question=question\n",
    "    )+'\\n'+answer_plugin.format(\n",
    "        answer=answer\n",
    "    )+'\\n'+answer_rating_plugin\n",
    "    print(f'[DEBUG] answer rate prompt: {prompt}')\n",
    "    answer_rating=send2GPT(prompt)\n",
    "    print(f'[DEBUG] answer rate identified: {answer_rating}')\n",
    "    print()\n",
    "    pricing+=priceEstimator(prompt, answer_rating)\n",
    "    \n",
    "    # aggregating price information, and move to the next text to parse\n",
    "    total_pricing+=pricing\n",
    "    print(f'[INFO] getting this data cost: {pricing}')\n",
    "    print(f'[INFO] current total cost: {total_pricing}')\n",
    "    \n",
    "    # append all acquired data to create the dataframe later, since anything may cause issue above\n",
    "    # then the lengths of arrays would be different\n",
    "    print('[INFO] appending data to dataframe')\n",
    "    qa_data['question'].append(question)\n",
    "    qa_data['asker'].append(asker)\n",
    "    qa_data['question_summary'].append(question_summary)\n",
    "    qa_data['answer'].append(answer)\n",
    "    qa_data['answerer'].append(answerer)\n",
    "    qa_data['answer_summary'].append(answer_summary)\n",
    "    qa_data['was_resolved'].append(was_resolved)\n",
    "    qa_data['answer_rating'].append(answer_rating)\n",
    "    qa_data['conversation'].append(text_data[i])\n",
    "    i+=1 # update the iteration of text to crawl\n",
    "    print('[INFO] data has been appended to the dataframe')\n",
    "\n",
    "    # wait to prevent being blocked\n",
    "    wait_time=np.random.randint(30,np.random.randint(60, min(120, 61+i//100)))\n",
    "    print(f'[DEBUG] pending with wait: {wait_time}s')\n",
    "    time.sleep(wait_time) # time for wait preventing OpenAI to be angry about the crawling here\n",
    "\n",
    "    # save checkpoints, with parsed data, and index of raw text parsed so far\n",
    "    print('[INFO] saving data checkpoints')\n",
    "    with open('data/checkpoints/chkpt.pickle','wb') as f:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                'qa_data':qa_data,\n",
    "                'idx':i\n",
    "            },f\n",
    "        )\n",
    "    pd.DataFrame(qa_data).to_csv(f'data/distilled_data/qa_{current_unixtime}.csv',index=False)\n",
    "    print('[INFO] checkpoint saved')\n",
    "    \n",
    "    print('----')\n",
    "    print()\n",
    "    time.sleep(6) # wait for 6 seconds then clear out the screen\n",
    "    clear_output()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
